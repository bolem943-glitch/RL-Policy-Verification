from sto_environment import GridWorld
from sto_agent import QLearningAgent
from sto_export_to_prism import export_to_prism
import random
import numpy as np
import time  # è®¡æ—¶

ACTION_SYMBOLS = {
    0: "â¬†ï¸",
    1: "â¡ï¸",
    2: "â¬‡ï¸",
    3: "â¬…ï¸"
}

def policies_equal(policy1, policy2, tol=1e-3):
    """æ¯”è¾ƒä¸¤ä¸ª 'state -> prob array' ç­–ç•¥æ˜¯å¦è¿‘ä¼¼ç›¸ç­‰ã€‚"""
    if policy1.keys() != policy2.keys():
        return False
    for state in policy1:
        if not np.allclose(policy1[state], policy2[state], atol=tol):
            return False
    return True

def main():
    # ---- total runtime start ----
    t0_total = time.perf_counter()

    # éšæœºç§å­
    seed = 42
    random.seed(seed)
    np.random.seed(seed)
    print(f"Random seed set to: {seed}")
    with open("used_seed.txt", "w") as f:
        f.write(str(seed))

    # ç¯å¢ƒé…ç½®
    size = 20
    start = (0, 0)
    goal = (19, 19)
    danger = [(i, j) for i in range(6, 10) for j in range(6, 10)]  # ä¸­å¤® danger åŒºåŸŸ

    env = GridWorld(size=size, start=start, goal=goal, danger=danger)
    agent = QLearningAgent(env, alpha=0.1, gamma=0.9, temperature=0.5)

    # ç»Ÿè®¡ä¿¡æ¯
    episodes = 5000
    max_steps = 500
    total_rewards = []
    success_count = 0
    danger_count = 0
    # converge_episode = None
    # prev_policy = None

    # ---- training runtime start ----
    t0_train = time.perf_counter()

    # è®­ç»ƒå¾ªç¯
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0.0
        visited_danger = False
        done = False
        steps = 0

        while not done and steps < max_steps:
            action = agent.choose_action(state)
            next_state, reward, done = env.step(action)
            agent.update(state, action, reward, next_state)

            total_reward += reward
            if next_state in danger:
                visited_danger = True

            state = next_state
            steps += 1

        total_rewards.append(total_reward)
        if state == goal:
            success_count += 1
        if visited_danger:
            danger_count += 1

        # è‹¥éœ€è¦â€œåˆ†å¸ƒæ”¶æ•›â€æ£€æµ‹ï¼Œå¯è§£é™¤ä»¥ä¸‹æ³¨é‡Š
        # current_policy = agent.extract_stochastic_policy()
        # if prev_policy is not None and policies_equal(current_policy, prev_policy) and converge_episode is None:
        #     converge_episode = episode
        # prev_policy = current_policy

    # ---- training runtime end ----
    t1_train = time.perf_counter()

    # æ±‡æ€»ç»Ÿè®¡
    avg_reward = float(np.mean(total_rewards))
    success_rate = 100.0 * success_count / episodes
    danger_rate = 100.0 * danger_count / episodes
    # converge_ep = converge_episode if converge_episode is not None else "Not Converged"

    print("\nâœ… Training Summary:")
    # print(f"- Avg Converge Ep   : {converge_ep}")
    print(f"- Avg Final Reward  : {avg_reward:.2f}")
    print(f"- Success Rate      : {success_rate:.2f}%")
    print(f"- Danger Rate       : {danger_rate:.2f}%")
    print(f"- Training Time     : {t1_train - t0_train:.3f} s")

    # æå– stochastic policyï¼ˆstate -> æ¦‚ç‡åˆ†å¸ƒï¼‰
    stoch_policy = agent.extract_stochastic_policy()

    # ç”¨ argmax(probabilities) æ¸²æŸ“äººç±»å¯è¯»çš„åŠ¨ä½œç¬¦å·
    print("\nğŸ“‹ Learned Policy Grid (argmax view):")
    for y in range(size):
        row = []
        for x in range(size):
            state = (x, y)
            if state == goal:
                row.append("âœ…")
            elif state in danger:
                row.append("â˜ ï¸")
            else:
                probs = stoch_policy.get(state)
                if probs is not None:
                    a = int(np.argmax(probs))
                else:
                    # è‹¥è¯¥çŠ¶æ€æœªå‡ºç°åœ¨ç­–ç•¥ä¸­ï¼Œåˆ™ç”¨å½“å‰ Q è¡¨çš„è´ªå©ªåŠ¨ä½œä½œä¸ºå›é€€
                    q_values = agent.q_table.get(state, np.zeros(env.action_dim))
                    a = int(np.argmax(q_values))
                row.append(ACTION_SYMBOLS[a])
        print(" ".join(row))

    # âœ… å¯¼å‡º PRISMï¼ˆä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒï¼‰
    export_to_prism(
        policy=stoch_policy,
        width=size,
        height=size,
        start=start,
        goal=goal,
        danger_list=danger,
        filename="policy_model.prism"
    )
    print("\nâœ… PRISM model exported as policy_model.prism")

    # ---- total runtime end ----
    t1_total = time.perf_counter()
    print(f"- Total Runtime     : {t1_total - t0_total:.3f} s")

if __name__ == "__main__":
    main()
